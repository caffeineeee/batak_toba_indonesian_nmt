{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1iroGyBn9lY"
      },
      "source": [
        "# TITLE: Batak Toba-Indonesian machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGimvZ2q2pkQ"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2ZI-TSNn9lQ"
      },
      "outputs": [],
      "source": [
        "# ! pip install transformers datasets evaluate sacrebleu tensorflow ipywidgets ipykernel transformers[torch] # If running Jupyter Notebook locally\n",
        "! pip install transformers datasets==2.14.6 evaluate sacrebleu transformers[torch] # If running using Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtgAcKFNBP-J"
      },
      "outputs": [],
      "source": [
        "# Add new secret (environment variable) by opening up the \"Secrets\" tab in the left-side panel\n",
        "# Name: \"HF_TOKEN\"\n",
        "# Value: Your HuggingFace access token (get it here https://huggingface.co/settings/tokens)\n",
        "\n",
        "# Or another way below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FQ9f0mMn9li"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "# login(\n",
        "#     token=\"YOUR_HUGGING_FACE_TOKEN_HERE\",\n",
        "#     add_to_git_credential=True\n",
        "#     )\n",
        "\n",
        "# # Disable caching on a global scale with disable_caching():\n",
        "# from datasets import disable_caching\n",
        "\n",
        "# disable_caching()\n",
        "# # When you disable caching, ðŸ¤— Datasets will no longer reload cached files when applying transforms to datasets. Any transform you apply on your dataset will be need to be reapplied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdSaC3GfYp4_"
      },
      "outputs": [],
      "source": [
        "# To be able to use indonlp/nusatranslation_mt, you need to install the following dependency: nusacrowd\n",
        "! pip install nusacrowd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "howEDH4a24gk"
      },
      "source": [
        "## 1. Batak Toba to Indonesian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLD-4Djfn9ll"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face Datasets\n",
        "dataset_btk_to_ind = load_dataset(\n",
        "    \"indonlp/nusatranslation_mt\",\n",
        "    \"nusatranslation_mt_btk_ind_source\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQp0CQpy2S0W"
      },
      "outputs": [],
      "source": [
        "dataset_btk_to_ind['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75fD8YG7d9ch"
      },
      "outputs": [],
      "source": [
        "# Rename 'text' to 'source' and 'label' to 'target' in the dataset\n",
        "dataset_btk_to_ind = dataset_btk_to_ind.rename_column(\n",
        "    \"text\",\n",
        "    \"source\"\n",
        "    )\n",
        "dataset_btk_to_ind = dataset_btk_to_ind.rename_column(\n",
        "    \"label\",\n",
        "    \"target\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji5BerEseQ-T"
      },
      "outputs": [],
      "source": [
        "# Verify the column names\n",
        "dataset_btk_to_ind['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgSqRCR8n9lp"
      },
      "source": [
        "### Dataset and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXcHB_Wun9lq"
      },
      "source": [
        "The next step is to load AutoTokenizer to process the Batak Toba-Indonesian language pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjSjfkNMip5K"
      },
      "outputs": [],
      "source": [
        "! pip install sentencepiece # For Facebook's NLLB models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euVK2LTtn9lq"
      },
      "outputs": [],
      "source": [
        "# # Run this to create and publish a new tokenizer, otherwise use the already published one.\n",
        "# # Add new src_lang code, e.g., \"btk_Latn\" to the model\n",
        "\n",
        "# # First, create a new tokenizer by loading the existing one and adding the new language code\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     \"facebook/nllb-200-distilled-600M\",\n",
        "#     src_lang=\"btk_Latn\",\n",
        "#     tgt_lang=\"ind_Latn\",\n",
        "#     additional_special_tokens=['btk_Latn']\n",
        "#     )\n",
        "\n",
        "# # Publish the new tokenizer to the Hub\n",
        "# tokenizer.push_to_hub(\n",
        "#     \"bbc-batak-toba-as-src-tokenizer\"\n",
        "#     )\n",
        "\n",
        "# # # Example of how others would use it:\n",
        "\n",
        "# # # Load the tokenizer from the Hub\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(\n",
        "# #     \"kepinsam/bbc-batak-toba-as-src-tokenizer\"\n",
        "# #     )\n",
        "\n",
        "# # # Use the tokenizer to tokenize a Batak Toba sentence\n",
        "# # sentence = \"Horas ma ho!\"\n",
        "# # tokens = tokenizer(\n",
        "# #     sentence\n",
        "# #     )\n",
        "# # print(\n",
        "# #     tokens\n",
        "# #     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7MI7ikSG4Ah"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"kepinsam/bbc-batak-toba-as-src-tokenizer\",\n",
        "    src_lang=\"btk_Latn\",\n",
        "    tgt_lang=\"ind_Latn\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Tajmnltn9lr"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [example for example in examples[\"source\"]]\n",
        "    targets = [example for example in examples[\"target\"]]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        text_target=targets,\n",
        "        max_length=64,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "        )\n",
        "    return model_inputs\n",
        "\n",
        "# speed up the map function by setting batched=True to process multiple elements of the dataset at once\n",
        "tokenized_dataset = dataset_btk_to_ind.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"id\", \"source\", \"target\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hclmvrxQn9lv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(\n",
        "        preds,\n",
        "        skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    labels = np.where(\n",
        "        labels != -100,\n",
        "        labels,\n",
        "        tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    decoded_labels = tokenizer.batch_decode(\n",
        "        labels,\n",
        "        skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(\n",
        "        decoded_preds,\n",
        "        decoded_labels\n",
        "        )\n",
        "\n",
        "    result = sacrebleu.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels\n",
        "        )\n",
        "\n",
        "    result = {\"sacrebleu\": result[\"score\"]}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pkz4Vzrn9lv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"facebook/nllb-200-distilled-600M\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6lWzcJmJt27"
      },
      "source": [
        "### Evaluation before training (Zero-shot translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvRLE9RUizAv"
      },
      "outputs": [],
      "source": [
        "# # Data collator is optional\n",
        "# from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# data_collator = DataCollatorForSeq2Seq(\n",
        "#     tokenizer=tokenizer,\n",
        "#     model=\"facebook/nllb-200-distilled-600M\"\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGye_s5gagi1"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "# training_args = Seq2SeqTrainingArguments(\n",
        "#     output_dir=\"bbc-to-ind-nmt-v1\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     per_device_eval_batch_size=16,\n",
        "\n",
        "#     # For optimizing resources usage (GPU, RAM, etc.)\n",
        "#     # ---\n",
        "#     predict_with_generate=True,\n",
        "#     optim=\"adamw_bnb_8bit\",\n",
        "#     fp16=True,\n",
        "#     save_total_limit=3,\n",
        "#     eval_accumulation_steps=4,\n",
        "#     # ---\n",
        "\n",
        "#     push_to_hub=False,\n",
        "#     )\n",
        "\n",
        "# # Create a Seq2SeqTrainer for evaluation only\n",
        "# trainer = Seq2SeqTrainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=None,  # Set train_dataset to None for evaluation\n",
        "#     # data_collator=data_collator,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     )\n",
        "\n",
        "# trainer.evaluate(\n",
        "#     eval_dataset=tokenized_dataset[\"test\"]\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXxMmTfUPsgt"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import evaluate\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=\"facebook/nllb-200-distilled-600M\"\n",
        "    )\n",
        "\n",
        "test_sentences = {\n",
        "    \"text1\": {\"source\": \"Unang godang manungkun ho.\", \"target\": \"Jangan banyak bertanya kau.\"},\n",
        "    \"text2\": {\n",
        "        \"source\": \"Asa adong hepengmu tu manuhor boras muse.\",\n",
        "        \"target\": \"Supaya ada uangmu untuk membeli beras lagi.\",\n",
        "    },\n",
        "    \"text3\": {\n",
        "        \"source\": \"Holong hian rohang hu tu ho dohot masihol hian au tu ho.\",\n",
        "        \"target\": \"Cinta sekali hatiku samamu dan rindu sekali aku samamu.\",\n",
        "    },\n",
        "    \"text4\": {\n",
        "        \"source\": \"Molo adong na salah, manang na hurang pambahenan ki, sai anju ma au.\",\n",
        "        \"target\": \"Kalau ada yang salah, atau yang kurang di perbuatan ku, selalu maafkanlah aku.\",\n",
        "    },\n",
        "    \"text5\": {\n",
        "        \"source\": \"Tarsongon bunga naung malos di ladang i, songon i ma rohang ki nunga malala, ndang hu rimpu songon i, dibahen ho holong ki gabe meam-meammu.\",\n",
        "        \"target\": \"Seperti bunga yang sudah layu di ladang, seperti itulah hatiku yang sudah hancur, tidak ku kira seperti ini, kau buat cintaku jadi main-mainmu.\",\n",
        "    },\n",
        "}\n",
        "\n",
        "translated_sentences = []\n",
        "\n",
        "# Perform translation and evaluation for each test sentence\n",
        "for key, text_dict in test_sentences.items():\n",
        "    source_text, translation, correct_translation = text_dict[\"source\"], \"\", text_dict[\"target\"]\n",
        "\n",
        "    # Translate the source text\n",
        "    translated_text = translator(\n",
        "        source_text,\n",
        "        src_lang=\"btk_Latn\",\n",
        "        tgt_lang=\"ind_Latn\"\n",
        "        )\n",
        "\n",
        "    translation = translated_text[0][\"translation_text\"]\n",
        "\n",
        "    # Calculate the SacreBLEU score\n",
        "    sacrebleu_score = sacrebleu.compute(\n",
        "        predictions=[translation],\n",
        "        references=[[correct_translation]]\n",
        "        )\n",
        "\n",
        "    sacrebleu_score = sacrebleu_score[\"score\"]\n",
        "\n",
        "    translated_sentences.append((\n",
        "        source_text,\n",
        "        translation,\n",
        "        correct_translation,\n",
        "        sacrebleu_score\n",
        "        ))\n",
        "\n",
        "# Print the results\n",
        "for i, (\n",
        "    source,\n",
        "    translation,\n",
        "    correct_translation,\n",
        "    sacrebleu_score\n",
        "    ) in enumerate(translated_sentences):\n",
        "    print(\n",
        "        f\"Sentence {i + 1}:\"\n",
        "        )\n",
        "    print(\n",
        "        \"Source: \", source\n",
        "        )\n",
        "    print(\n",
        "        \"Target: \",\n",
        "        correct_translation\n",
        "        )\n",
        "    print(\n",
        "        \"Prediction: \", translation\n",
        "        )\n",
        "    print(\n",
        "        \"SacreBLEU Score: \",\n",
        "        sacrebleu_score\n",
        "        )\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0fNAEHhCFKA"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt-OfBcSCtMs"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72BKKLdLH_DB"
      },
      "outputs": [],
      "source": [
        "! pip install bitsandbytes # For 'adamw_bnb_8bit' optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSTNLqCIBMYH"
      },
      "outputs": [],
      "source": [
        "model.config.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEEI441EwGaX"
      },
      "outputs": [],
      "source": [
        "# # Data collator is optional\n",
        "# from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# data_collator = DataCollatorForSeq2Seq(\n",
        "#     tokenizer=tokenizer,\n",
        "#     model=\"facebook/nllb-200-distilled-600M\"\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXkH9Daen9lw"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"bbc-to-ind-nmt-v2\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    # ---\n",
        "\n",
        "    # More influential hyperparameters\n",
        "    num_train_epochs=5, # iterate on [5, 10] # Higher means longer training time\n",
        "    per_device_train_batch_size=8, # iterate on [4, 8, 16, 32] # Higher needs higher GPU RAM\n",
        "\n",
        "    # Less influential hyperparameters\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.3,\n",
        "    warmup_ratio=0.1,\n",
        "    per_device_eval_batch_size=16,\n",
        "    # ---\n",
        "\n",
        "\n",
        "    # For optimizing resources usage (System RAM, GPU RAM, Disk, etc.)\n",
        "    # ---\n",
        "    predict_with_generate=True,\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    fp16=True,\n",
        "    save_total_limit=1,\n",
        "    eval_accumulation_steps=4,\n",
        "    # ---\n",
        "\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    # data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL6V3RlDn9lw"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrvgsYA6KvxG"
      },
      "source": [
        "### Evaluation after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnH5R8xen9lw"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import evaluate\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     \"kepinsam/bbc-batak-toba-as-src-tokenizer\",\n",
        "#     src_lang=\"btk_Latn\",\n",
        "#     tgt_lang=\"ind_Latn\"\n",
        "#     )\n",
        "\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=\"kepinsam/bbc-to-ind-nmt-v2\",\n",
        "    # decoder_start_token_id=tokenizer.bos_token_id,\n",
        "    )\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "test_sentences = {\n",
        "    \"text1\": {\"source\": \"Unang godang manungkun ho.\", \"target\": \"Jangan banyak bertanya kau.\"},\n",
        "    \"text2\": {\n",
        "        \"source\": \"Asa adong hepengmu tu manuhor boras muse.\",\n",
        "        \"target\": \"Supaya ada uangmu untuk membeli beras lagi.\",\n",
        "    },\n",
        "    \"text3\": {\n",
        "        \"source\": \"Holong hian rohang hu tu ho dohot masihol hian au tu ho.\",\n",
        "        \"target\": \"Cinta sekali hatiku samamu dan rindu sekali aku samamu.\",\n",
        "    },\n",
        "    \"text4\": {\n",
        "        \"source\": \"Molo adong na salah, manang na hurang pambahenan ki, sai anju ma au.\",\n",
        "        \"target\": \"Kalau ada yang salah, atau yang kurang di perbuatan ku, selalu maafkanlah aku.\",\n",
        "    },\n",
        "    \"text5\": {\n",
        "        \"source\": \"Tarsongon bunga naung malos di ladang i, songon i ma rohang ki nunga malala, ndang hu rimpu songon i, dibahen ho holong ki gabe meam-meammu.\",\n",
        "        \"target\": \"Seperti bunga yang sudah layu di ladang, seperti itulah hatiku yang sudah hancur, tidak ku kira seperti ini, kau buat cintaku jadi main-mainmu.\",\n",
        "    },\n",
        "}\n",
        "\n",
        "translated_sentences = []\n",
        "\n",
        "# Perform translation and evaluation for each test sentence\n",
        "for key, text_dict in test_sentences.items():\n",
        "    source_text, translation, correct_translation = text_dict[\"source\"], \"\", text_dict[\"target\"]\n",
        "\n",
        "    # Translate the source text\n",
        "    translated_text = translator(\n",
        "        source_text,\n",
        "        src_lang=\"btk_Latn\",\n",
        "        tgt_lang=\"ind_Latn\"\n",
        "        )\n",
        "    translation = translated_text[0][\"translation_text\"]\n",
        "\n",
        "    # Calculate the SacreBLEU score\n",
        "    sacrebleu_score = sacrebleu.compute(\n",
        "        predictions=[translation],\n",
        "        references=[[correct_translation]]\n",
        "        )\n",
        "\n",
        "    sacrebleu_score = sacrebleu_score[\"score\"]\n",
        "\n",
        "    translated_sentences.append((\n",
        "        source_text,\n",
        "        translation,\n",
        "        correct_translation,\n",
        "        sacrebleu_score\n",
        "        ))\n",
        "\n",
        "# Print the results\n",
        "for i, (\n",
        "    source,\n",
        "    translation,\n",
        "    correct_translation,\n",
        "    sacrebleu_score\n",
        "    ) in enumerate(translated_sentences):\n",
        "    print(\n",
        "        f\"Sentence {i + 1}:\"\n",
        "        )\n",
        "    print(\n",
        "        \"Source: \",\n",
        "        source\n",
        "        )\n",
        "    print(\n",
        "        \"Target: \",\n",
        "        correct_translation\n",
        "        )\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        translation\n",
        "        )\n",
        "    print(\n",
        "        \"SacreBLEU Score: \",\n",
        "        sacrebleu_score\n",
        "        )\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3EK6Gr53R9B"
      },
      "source": [
        "## 2. Indonesian to Batak Toba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVZugjl53hwq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face Datasets\n",
        "dataset_ind_to_btk = load_dataset(\n",
        "    \"indonlp/nusatranslation_mt\",\n",
        "    \"nusatranslation_mt_btk_ind_source\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r-cLdI73hwz"
      },
      "outputs": [],
      "source": [
        "dataset_ind_to_btk['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ6psh9X0mws"
      },
      "outputs": [],
      "source": [
        "# Rename 'label' to 'source' and 'text' to 'target' in the dataset\n",
        "dataset_ind_to_btk = dataset_ind_to_btk.rename_column(\n",
        "    \"label\",\n",
        "    \"source\"\n",
        "    )\n",
        "dataset_ind_to_btk = dataset_ind_to_btk.rename_column(\n",
        "    \"text\",\n",
        "    \"target\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6SffebL0mwt"
      },
      "outputs": [],
      "source": [
        "# Verify the column names\n",
        "dataset_ind_to_btk['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMr8S-DS0mwt"
      },
      "source": [
        "### Dataset and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLpsDeiK0mwt"
      },
      "source": [
        "The next step is to load an AutoTokenizer to process the Batak Toba-Indonesian language pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKAWIGiV0mwu"
      },
      "outputs": [],
      "source": [
        "! pip install sentencepiece # For Facebook's NLLB models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy9xUdZpG0Gj"
      },
      "outputs": [],
      "source": [
        "# # Run this to create and publish a new tokenizer, otherwise use the already published one.\n",
        "# # Add new tgt_lang code, e.g., \"btk_Latn\" to the model\n",
        "\n",
        "# # First, create a new tokenizer by loading the existing one and adding the new language code\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     \"facebook/nllb-200-distilled-600M\",\n",
        "#     src_lang=\"ind_Latn\",\n",
        "#     tgt_lang=\"btk_Latn\",\n",
        "#     additional_special_tokens=['btk_Latn']\n",
        "#     )\n",
        "\n",
        "# # Publish the new tokenizer to the Hub\n",
        "# tokenizer.push_to_hub(\n",
        "#     \"bbc-batak-toba-as-tgt-tokenizer\"\n",
        "#     )\n",
        "\n",
        "# # # Example of how others would use it:\n",
        "\n",
        "# # # Load the tokenizer from the Hub\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(\n",
        "# #     \"kepinsam/bbc-batak-toba-as-tgt-tokenizer\"\n",
        "# #     )\n",
        "\n",
        "# # # Use the tokenizer to tokenize a Batak Toba sentence\n",
        "# # sentence = \"Horas ma ho!\"\n",
        "# # tokens = tokenizer(\n",
        "# #     sentence\n",
        "# #     )\n",
        "# # print(\n",
        "# #     tokens\n",
        "# #     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VDQqsW20mwu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"kepinsam/bbc-batak-toba-as-tgt-tokenizer\",\n",
        "    src_lang=\"ind_Latn\",\n",
        "    tgt_lang=\"btk_Latn\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r18uVFVk0mwu"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [example for example in examples[\"source\"]]\n",
        "    targets = [example for example in examples[\"target\"]]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        text_target=targets,\n",
        "        max_length=64,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "        )\n",
        "    return model_inputs\n",
        "\n",
        "# speed up the map function by setting batched=True to process multiple elements of the dataset at once\n",
        "tokenized_dataset = dataset_ind_to_btk.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"id\", \"source\", \"target\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIFOJ5h0mww"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(\n",
        "        preds,\n",
        "        skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    labels = np.where(\n",
        "        labels != -100,\n",
        "        labels,\n",
        "        tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    decoded_labels = tokenizer.batch_decode(\n",
        "        labels,\n",
        "        skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(\n",
        "        decoded_preds,\n",
        "        decoded_labels\n",
        "        )\n",
        "\n",
        "    result = sacrebleu.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels\n",
        "        )\n",
        "\n",
        "    result = {\"sacrebleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVqBMXaJ0mww"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"facebook/nllb-200-distilled-600M\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhIMRaXG0mww"
      },
      "source": [
        "### Evaluation before training (Zero-shot translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqp6nRuOl9Xm"
      },
      "outputs": [],
      "source": [
        "# # Data collator is optional\n",
        "# from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# data_collator = DataCollatorForSeq2Seq(\n",
        "#     tokenizer=tokenizer,\n",
        "#     model=\"facebook/nllb-200-distilled-600M\",\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtnABQL50mwx"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "# training_args = Seq2SeqTrainingArguments(\n",
        "#     output_dir=\"ind-to-bbc-nmt-v2\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     per_device_eval_batch_size=8,\n",
        "\n",
        "#     # For optimizing resources usage (GPU, RAM, etc.)\n",
        "#     # ---\n",
        "#     predict_with_generate=True,\n",
        "#     optim=\"adamw_bnb_8bit\",\n",
        "#     fp16=True,\n",
        "#     save_total_limit=3,\n",
        "#     eval_accumulation_steps=4,\n",
        "#     # ---\n",
        "\n",
        "#     push_to_hub=False,\n",
        "# )\n",
        "\n",
        "# # Create a Seq2SeqTrainer for evaluation only\n",
        "# trainer = Seq2SeqTrainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=None,  # Set train_dataset to None for evaluation\n",
        "#     # data_collator=data_collator,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     )\n",
        "\n",
        "# trainer.evaluate(\n",
        "#     eval_dataset=tokenized_dataset[\"test\"]\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAwmX1RC0mwx"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "# import evaluate\n",
        "\n",
        "# sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# translator = pipeline(\n",
        "#     \"translation\",\n",
        "#     model=\"facebook/nllb-200-distilled-600M\"\n",
        "#     )\n",
        "\n",
        "# test_sentences = {\n",
        "#     \"text1\": {\n",
        "#         \"source\": \"Turun disini lah kita.\",\n",
        "#         \"target\": \"Turun dison ma hita.\",\n",
        "#         },\n",
        "#     \"text2\": {\n",
        "#         \"source\": \"Kalau tidak memasak kau, ayolah makan ke luar kita.\",\n",
        "#         \"target\": \"Molo ndang mangalompa ho, eta ma mangan tu kaluar hita.\",\n",
        "#     },\n",
        "#     \"text3\": {\n",
        "#         \"source\": \"Kalau pergi kau, hatiku pergi bersamamu.\",\n",
        "#         \"target\": \"Molo lao ho, rohakku dohot mai lao.\",\n",
        "#     },\n",
        "#     \"text4\": {\n",
        "#         \"source\": \"Biarpun meminta kau, tidak mau aku.\",\n",
        "#         \"target\": \"Agia mangido pe ho, ndang olo au.\",\n",
        "#     },\n",
        "#     \"text5\": {\n",
        "#         \"source\": \"Dahulu hiduplah seorang saja yang Bernama Raja Rahat yang berkuasa di Samosir.\",\n",
        "#         \"target\": \"Na jolo adong ma raja na margoar Raja Rahat na marhuaso i Samosir.\",\n",
        "#     },\n",
        "#     \"text6\": {\n",
        "#         \"source\": \"Biarlah orang lain memuji engkau dan bukan mulutmu, orang yang tidak kau kenal dan bukan bibirmu sendiri. Batu adalah berat dan pasir pun ada beratnya, tetapi lebih berat dari kedua-duanya adalah sakit hati terhadap orang bodoh.\",\n",
        "#         \"target\": \"Halak na asing tagonan mamuji ho, unang tung pamanganmu sandiri, halak sileban tagonan, unang bibirmu sandiri. Dokdok do batu, jala borat horsik, alai dumokdok sian duansa do anggo hamurhingon ni halak na oto.\",\n",
        "#     },\n",
        "# }\n",
        "\n",
        "# translated_sentences = []\n",
        "\n",
        "# # Perform translation and evaluation for each test sentence\n",
        "# for key, text_dict in test_sentences.items():\n",
        "#     source_text, translation, correct_translation = text_dict[\"source\"], \"\", text_dict[\"target\"]\n",
        "\n",
        "#     # Translate the source text\n",
        "#     translated_text = translator(\n",
        "#         source_text,\n",
        "#         src_lang=\"ind_Latn\",\n",
        "#         tgt_lang=\"btk_Latn\"\n",
        "#         )\n",
        "#     translation = translated_text[0][\"translation_text\"]\n",
        "\n",
        "#     # Calculate the SacreBLEU score\n",
        "#     sacrebleu_score = sacrebleu.compute(\n",
        "#         predictions=[translation],\n",
        "#         references=[[correct_translation]]\n",
        "#         )\n",
        "\n",
        "#     sacrebleu_score = sacrebleu_score[\"score\"]\n",
        "\n",
        "#     translated_sentences.append((\n",
        "#         source_text,\n",
        "#         translation,\n",
        "#         correct_translation,\n",
        "#         sacrebleu_score\n",
        "#         ))\n",
        "\n",
        "# # Print the results\n",
        "# for i, (\n",
        "#     source,\n",
        "#     translation,\n",
        "#     correct_translation,\n",
        "#     sacrebleu_score\n",
        "#     ) in enumerate(translated_sentences):\n",
        "#     print(\n",
        "#         f\"Sentence {i + 1}:\"\n",
        "#         )\n",
        "#     print(\n",
        "#         \"Source: \",\n",
        "#         source\n",
        "#         )\n",
        "#     print(\n",
        "#         \"Target: \",\n",
        "#         correct_translation\n",
        "#         )\n",
        "#     print(\n",
        "#         \"Prediction: \",\n",
        "#         translation\n",
        "#         )\n",
        "#     print(\n",
        "#         \"SacreBLEU Score: \",\n",
        "#         sacrebleu_score\n",
        "#         )\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKXosw5dB3yp"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGMxo2jkCw6W"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRP5p4KO0mwx"
      },
      "outputs": [],
      "source": [
        "! pip install bitsandbytes # For 'adamw_bnb_8bit' optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UqYKM2YBEFg"
      },
      "outputs": [],
      "source": [
        "model.config.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cJquM0MwpGZ"
      },
      "outputs": [],
      "source": [
        "# # Data collator is optional\n",
        "# from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# data_collator = DataCollatorForSeq2Seq(\n",
        "#     tokenizer=tokenizer,\n",
        "#     model=\"facebook/nllb-200-distilled-600M\"\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dKFYkrI0mwy"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"ind-to-bbc-nmt-v2\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    # ---\n",
        "\n",
        "    # More influential hyperparameters\n",
        "    num_train_epochs=5, # iterate on [5, 10] # Higher means longer training time\n",
        "    per_device_train_batch_size=8, # iterate on [4, 8, 16, 32] # Higher needs higher GPU RAM\n",
        "\n",
        "    # Less influential hyperparameters\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.3,\n",
        "    warmup_ratio=0.1,\n",
        "    per_device_eval_batch_size=16,\n",
        "    # ---\n",
        "\n",
        "    # For optimizing resources usage (System RAM, GPU RAM, Disk, etc.)\n",
        "    # ---\n",
        "    predict_with_generate=True,\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    fp16=True,\n",
        "    save_total_limit=1,\n",
        "    eval_accumulation_steps=4,\n",
        "    # ---\n",
        "\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    # data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzc8DKYfoaqj"
      },
      "outputs": [],
      "source": [
        "# from transformers import GenerationConfig\n",
        "\n",
        "# # Create a GenerationConfig object with your desired parameters\n",
        "# generation_config = GenerationConfig(\n",
        "#     max_length=200,\n",
        "#     push_to_hub=True\n",
        "#     # Add other generation parameters as needed\n",
        "# )\n",
        "\n",
        "# # Save the GenerationConfig with your model\n",
        "# model.generation_config = generation_config\n",
        "# model.save_pretrained(\n",
        "#     \"ind-to-bbc-nmt-v2\",\n",
        "#     generation_config=generation_config\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEzJW7wW0mwy"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Pr84rH0mwy"
      },
      "source": [
        "### Evaluation after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kisx7y9-0mwy"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import evaluate\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     \"kepinsam/bbc-batak-toba-as-tgt-tokenizer\",\n",
        "#     src_lang=\"ind_Latn\",\n",
        "#     tgt_lang=\"btk_Latn\"\n",
        "#     )\n",
        "\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=\"kepinsam/ind-to-bbc-nmt-v2\",\n",
        "    # decoder_start_token_id=tokenizer.bos_token_id,\n",
        "    )\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "test_sentences = {\n",
        "    \"text1\": {\n",
        "        \"source\": \"Turun disini lah kita.\",\n",
        "        \"target\": \"Turun dison ma hita.\",\n",
        "        },\n",
        "    \"text2\": {\n",
        "        \"source\": \"Kalau tidak memasak kau, ayolah makan ke luar kita.\",\n",
        "        \"target\": \"Molo ndang mangalompa ho, eta ma mangan tu kaluar hita.\",\n",
        "    },\n",
        "    \"text3\": {\n",
        "        \"source\": \"Kalau pergi kau, hatiku pergi bersamamu.\",\n",
        "        \"target\": \"Molo lao ho, rohakku dohot mai lao.\",\n",
        "    },\n",
        "    \"text4\": {\n",
        "        \"source\": \"Biarpun meminta kau, tidak mau aku.\",\n",
        "        \"target\": \"Agia mangido pe ho, ndang olo au.\",\n",
        "    },\n",
        "    \"text5\": {\n",
        "        \"source\": \"Dahulu hiduplah seorang saja yang Bernama Raja Rahat yang berkuasa di Samosir.\",\n",
        "        \"target\": \"Na jolo adong ma raja na margoar Raja Rahat na marhuaso i Samosir.\",\n",
        "    },\n",
        "    \"text6\": {\n",
        "        \"source\": \"Biarlah orang lain memuji engkau dan bukan mulutmu, orang yang tidak kau kenal dan bukan bibirmu sendiri. Batu adalah berat dan pasir pun ada beratnya, tetapi lebih berat dari kedua-duanya adalah sakit hati terhadap orang bodoh.\",\n",
        "        \"target\": \"Halak na asing tagonan mamuji ho, unang tung pamanganmu sandiri, halak sileban tagonan, unang bibirmu sandiri. Dokdok do batu, jala borat horsik, alai dumokdok sian duansa do anggo hamurhingon ni halak na oto.\",\n",
        "    },\n",
        "}\n",
        "\n",
        "translated_sentences = []\n",
        "\n",
        "# Perform translation and evaluation for each test sentence\n",
        "for key, text_dict in test_sentences.items():\n",
        "    source_text, translation, correct_translation = text_dict[\"source\"], \"\", text_dict[\"target\"]\n",
        "\n",
        "    # Translate the source text\n",
        "    translated_text = translator(\n",
        "        source_text,\n",
        "        src_lang=\"ind_Latn\",\n",
        "        tgt_lang=\"btk_Latn\"\n",
        "        )\n",
        "    translation = translated_text[0][\"translation_text\"]\n",
        "\n",
        "    # Calculate the SacreBLEU score\n",
        "    sacrebleu_score = sacrebleu.compute(\n",
        "        predictions=[translation],\n",
        "        references=[[correct_translation]]\n",
        "        )\n",
        "\n",
        "    sacrebleu_score = sacrebleu_score[\"score\"]\n",
        "\n",
        "    translated_sentences.append((\n",
        "        source_text,\n",
        "        translation,\n",
        "        correct_translation,\n",
        "        sacrebleu_score\n",
        "        ))\n",
        "\n",
        "# Print the results\n",
        "for i, (\n",
        "    source,\n",
        "    translation,\n",
        "    correct_translation,\n",
        "    sacrebleu_score\n",
        "    ) in enumerate(translated_sentences):\n",
        "    print(\n",
        "        f\"Sentence {i + 1}:\"\n",
        "        )\n",
        "    print(\n",
        "        \"Source: \",\n",
        "        source\n",
        "        )\n",
        "    print(\n",
        "        \"Target: \",\n",
        "        correct_translation\n",
        "        )\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        translation\n",
        "        )\n",
        "    print(\n",
        "        \"SacreBLEU Score: \",\n",
        "        sacrebleu_score\n",
        "        )\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
